{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo9L2IWBZXwq"
      },
      "source": [
        "Installation des packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU3tbMi5ZPru"
      },
      "outputs": [],
      "source": [
        "!pip install -q numpy pandas scikit-learn lightgbm xgboost catboost optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohfr7LDdZb8m"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFmlwSX2Zd7i"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import csv\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "print(\"Imports completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InK97CDvZh9v"
      },
      "source": [
        " Fonctions de nettoyage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDdN1K_rZkf1"
      },
      "outputs": [],
      "source": [
        "_STR_NAN_TOKENS = {\"nan\", \"NaN\", \"None\", \"none\", \"\"}\n",
        "\n",
        "def _strip_norm(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Normalize string tokens to NaN\"\"\"\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if df[c].dtype == object:\n",
        "            s = df[c].astype(str).str.strip()\n",
        "            df[c] = s.replace(list(_STR_NAN_TOKENS), np.nan)\n",
        "    return df\n",
        "\n",
        "def _add_missing_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Add missing indicator flags\"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"is_missing_indicator\"] = df[\"indicator\"].isna().astype(int) if \"indicator\" in df else 0\n",
        "    df[\"is_missing_indicator_description\"] = (\n",
        "        df[\"indicator_description\"].isna().astype(int) if \"indicator_description\" in df else 0\n",
        "    )\n",
        "    df[\"is_missing_time_observed\"] = df[\"time_observed\"].isna().astype(int) if \"time_observed\" in df else 0\n",
        "    return df\n",
        "\n",
        "def _impute_text_unknown(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Impute 'Unknown' for missing text columns\"\"\"\n",
        "    df = df.copy()\n",
        "    for col in [\"indicator\", \"indicator_description\", \"time_observed\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(\"Unknown\")\n",
        "    return df\n",
        "\n",
        "def _canonical_columns_with_flags(original_csv_path: str) -> list:\n",
        "    \"\"\"Get canonical column order\"\"\"\n",
        "    base_cols = list(pd.read_csv(original_csv_path, nrows=0).columns)\n",
        "    return base_cols + [\n",
        "        \"is_missing_indicator\",\n",
        "        \"is_missing_indicator_description\",\n",
        "        \"is_missing_time_observed\",\n",
        "    ]\n",
        "\n",
        "def _save_canonical(df: pd.DataFrame, out_path: str) -> None:\n",
        "    \"\"\"Save DataFrame with canonical format\"\"\"\n",
        "    df.to_csv(\n",
        "        out_path,\n",
        "        index=False,\n",
        "        encoding=\"utf-8\",\n",
        "        lineterminator=\"\\n\",\n",
        "        quoting=csv.QUOTE_MINIMAL,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piyQ16W4Zpft"
      },
      "source": [
        " Fonction principale de nettoyage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7erCSe_nZrCg"
      },
      "outputs": [],
      "source": [
        "def rebuild_clean_files(\n",
        "    raw_train_path: str,\n",
        "    raw_test_path: str,\n",
        "    out_train_clean_path: str,\n",
        "    out_test_clean_path: str,\n",
        ") -> Tuple[Tuple[int, int], Tuple[int, int]]:\n",
        "    \"\"\"Clean raw CSV files and save cleaned versions\"\"\"\n",
        "\n",
        "    train = pd.read_csv(raw_train_path)\n",
        "    test = pd.read_csv(raw_test_path)\n",
        "\n",
        "    train = _strip_norm(train)\n",
        "    test = _strip_norm(test)\n",
        "\n",
        "    train = _add_missing_flags(train)\n",
        "    test = _add_missing_flags(test)\n",
        "\n",
        "    train = _impute_text_unknown(train)\n",
        "    test = _impute_text_unknown(test)\n",
        "\n",
        "    if \"ID\" in train.columns:\n",
        "        train[\"ID\"] = train[\"ID\"].astype(str).str.strip()\n",
        "    if \"ID\" in test.columns:\n",
        "        test[\"ID\"] = test[\"ID\"].astype(str).str.strip()\n",
        "    if \"Target\" in train.columns and train[\"Target\"].dtype == object:\n",
        "        train[\"Target\"] = train[\"Target\"].str.upper().str.strip()\n",
        "\n",
        "    train_cols = _canonical_columns_with_flags(raw_train_path)\n",
        "    test_cols = _canonical_columns_with_flags(raw_test_path)\n",
        "\n",
        "    train = train.reindex(columns=train_cols)\n",
        "    test = test.reindex(columns=test_cols)\n",
        "\n",
        "    _save_canonical(train, out_train_clean_path)\n",
        "    _save_canonical(test, out_test_clean_path)\n",
        "\n",
        "    return train.shape, test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgrgcTQmZuNW"
      },
      "source": [
        "Exécution du nettoyage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iKX9FU6ZvJe"
      },
      "outputs": [],
      "source": [
        "print(\"Starting data cleaning...\")\n",
        "train_shape, test_shape = rebuild_clean_files(\n",
        "    raw_train_path=\"/content/train.csv\",\n",
        "    raw_test_path=\"/content/test.csv\",\n",
        "    out_train_clean_path=\"train_clean.csv\",\n",
        "    out_test_clean_path=\"test_clean.csv\",\n",
        ")\n",
        "print(f\"Clean files generated - Train: {train_shape}, Test: {test_shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84NgAdNxZy9l"
      },
      "source": [
        " Chargement et exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAOtbIsbZ0jJ"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train_clean.csv')\n",
        "test = pd.read_csv('test_clean.csv')\n",
        "\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(train['Target'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfKwJ7NlZ61w"
      },
      "source": [
        "Fonction de feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-9os705Z8zQ"
      },
      "outputs": [],
      "source": [
        "def advanced_features(df, is_train=True, text_vectorizer=None, fit_text=False):\n",
        "    \"\"\"Advanced feature engineering\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Time features\n",
        "    df['prediction_time'] = pd.to_datetime(df['prediction_time'])\n",
        "    df['hour'] = df['prediction_time'].dt.hour\n",
        "    df['day'] = df['prediction_time'].dt.day\n",
        "    df['month'] = df['prediction_time'].dt.month\n",
        "    df['dayofweek'] = df['prediction_time'].dt.dayofweek\n",
        "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
        "    df['week_of_year'] = df['prediction_time'].dt.isocalendar().week\n",
        "    df['day_of_year'] = df['prediction_time'].dt.dayofyear\n",
        "\n",
        "    # Cyclical features\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
        "\n",
        "    # Time of day indicators\n",
        "    df['is_morning'] = ((df['hour'] >= 6) & (df['hour'] < 12)).astype(int)\n",
        "    df['is_afternoon'] = ((df['hour'] >= 12) & (df['hour'] < 18)).astype(int)\n",
        "    df['is_evening'] = ((df['hour'] >= 18) & (df['hour'] < 22)).astype(int)\n",
        "    df['is_night'] = ((df['hour'] >= 22) | (df['hour'] < 6)).astype(int)\n",
        "\n",
        "    # Seasonal patterns\n",
        "    df['rainy_main'] = ((df['month'] >= 4) & (df['month'] <= 6)).astype(int)\n",
        "    df['rainy_secondary'] = ((df['month'] >= 9) & (df['month'] <= 11)).astype(int)\n",
        "    df['dry_season'] = ((df['rainy_main'] == 0) & (df['rainy_secondary'] == 0)).astype(int)\n",
        "\n",
        "    # Feature interactions\n",
        "    df['conf_x_intensity'] = df['confidence'] * df['predicted_intensity']\n",
        "    df['conf_squared'] = df['confidence'] ** 2\n",
        "    df['conf_cubed'] = df['confidence'] ** 3\n",
        "    df['intensity_x_forecast'] = df['predicted_intensity'] * df['forecast_length']\n",
        "    df['conf_x_forecast'] = df['confidence'] * df['forecast_length']\n",
        "    df['conf_x_hour'] = df['confidence'] * df['hour']\n",
        "    df['conf_x_rainy'] = df['confidence'] * (df['rainy_main'] + df['rainy_secondary'])\n",
        "\n",
        "    df['conf_bins'] = pd.cut(df['confidence'], bins=10, labels=False)\n",
        "\n",
        "    # Missing indicators\n",
        "    df['has_indicator'] = df['indicator'].notna().astype(int)\n",
        "    df['has_indicator_desc'] = df['indicator_description'].notna().astype(int)\n",
        "    df['has_time_observed'] = df['time_observed'].notna().astype(int)\n",
        "    df['missing_count'] = (3 - df['has_indicator'] - df['has_indicator_desc'] - df['has_time_observed'])\n",
        "\n",
        "    # Text features\n",
        "    if 'indicator_description' in df.columns:\n",
        "        if fit_text:\n",
        "            text_data = df['indicator_description'].fillna('missing').astype(str)\n",
        "            text_vectorizer = TfidfVectorizer(\n",
        "                max_features=50,\n",
        "                ngram_range=(1, 2),\n",
        "                min_df=2,\n",
        "                stop_words='english'\n",
        "            )\n",
        "            text_matrix = text_vectorizer.fit_transform(text_data)\n",
        "            text_df = pd.DataFrame(\n",
        "                text_matrix.toarray(),\n",
        "                columns=[f'tfidf_{i}' for i in range(text_matrix.shape[1])]\n",
        "            )\n",
        "            df = pd.concat([df.reset_index(drop=True), text_df], axis=1)\n",
        "        elif text_vectorizer is not None:\n",
        "            text_data = df['indicator_description'].fillna('missing').astype(str)\n",
        "            text_matrix = text_vectorizer.transform(text_data)\n",
        "            text_df = pd.DataFrame(\n",
        "                text_matrix.toarray(),\n",
        "                columns=[f'tfidf_{i}' for i in range(text_matrix.shape[1])]\n",
        "            )\n",
        "            df = pd.concat([df.reset_index(drop=True), text_df], axis=1)\n",
        "\n",
        "    return df, text_vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0ZPHxELaBWV"
      },
      "source": [
        "Application du feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOMc18SFaFOa"
      },
      "outputs": [],
      "source": [
        "print(\"Applying feature engineering...\")\n",
        "train, text_vec = advanced_features(train, is_train=True, fit_text=True)\n",
        "test, _ = advanced_features(test, is_train=False, text_vectorizer=text_vec)\n",
        "print(\"Feature engineering completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAB869sWaKy7"
      },
      "source": [
        "Agrégations statistiques - User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9ooZM2GaMvX"
      },
      "outputs": [],
      "source": [
        "print(\"Creating user statistics...\")\n",
        "train_original = train.copy()\n",
        "test_original = test.copy()\n",
        "\n",
        "user_stats_global = train.groupby('user_id').agg({\n",
        "    'confidence': ['mean', 'std', 'min', 'max', 'count'],\n",
        "    'predicted_intensity': ['mean', 'sum'],\n",
        "    'forecast_length': 'mean'\n",
        "}).reset_index()\n",
        "user_stats_global.columns = ['user_id'] + [f'user_{col[0]}_{col[1]}' for col in user_stats_global.columns[1:]]\n",
        "user_stats_global['user_confidence_std'] = user_stats_global['user_confidence_std'].fillna(0)\n",
        "\n",
        "print(f\"User stats shape: {user_stats_global.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bOJxHaBaQn0"
      },
      "source": [
        " Agrégations statistiques - Community & District"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV5YB2slaSkT"
      },
      "outputs": [],
      "source": [
        "print(\"Creating community and district statistics...\")\n",
        "\n",
        "comm_stats_global = train.groupby('community').agg({\n",
        "    'confidence': ['mean', 'std'],\n",
        "    'predicted_intensity': 'mean',\n",
        "    'Target': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'NORAIN'\n",
        "}).reset_index()\n",
        "comm_stats_global.columns = ['community', 'comm_conf_mean', 'comm_conf_std', 'comm_intensity_mean', 'comm_target_mode']\n",
        "comm_stats_global['comm_conf_std'] = comm_stats_global['comm_conf_std'].fillna(0)\n",
        "\n",
        "dist_stats_global = train.groupby('district').agg({\n",
        "    'confidence': 'mean',\n",
        "    'predicted_intensity': 'mean',\n",
        "    'Target': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'NORAIN'\n",
        "}).reset_index()\n",
        "dist_stats_global.columns = ['district', 'dist_conf_mean', 'dist_intensity_mean', 'dist_target_mode']\n",
        "\n",
        "print(f\"Community stats: {comm_stats_global.shape}, District stats: {dist_stats_global.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n29vYnj-aX8i"
      },
      "source": [
        "Agrégations temporelles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EU-U1EXacAc"
      },
      "outputs": [],
      "source": [
        "print(\"Creating time-based statistics...\")\n",
        "\n",
        "train['hour_block'] = (train['hour'] // 4)\n",
        "test['hour_block'] = (test['hour'] // 4)\n",
        "\n",
        "time_stats_global = train.groupby('hour_block').agg({\n",
        "    'confidence': 'mean',\n",
        "    'Target': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'NORAIN'\n",
        "}).reset_index()\n",
        "time_stats_global.columns = ['hour_block', 'hourblock_conf_mean', 'hourblock_target_mode']\n",
        "\n",
        "print(f\"Time stats shape: {time_stats_global.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42zcNYBQagIB"
      },
      "source": [
        "Fusion des agrégations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCpvRWNIahzM"
      },
      "outputs": [],
      "source": [
        "print(\"Merging aggregations...\")\n",
        "\n",
        "train = train.merge(user_stats_global, on='user_id', how='left')\n",
        "train = train.merge(comm_stats_global, on='community', how='left')\n",
        "train = train.merge(dist_stats_global, on='district', how='left')\n",
        "train = train.merge(time_stats_global, on='hour_block', how='left')\n",
        "\n",
        "test = test.merge(user_stats_global, on='user_id', how='left')\n",
        "test = test.merge(comm_stats_global, on='community', how='left')\n",
        "test = test.merge(dist_stats_global, on='district', how='left')\n",
        "test = test.merge(time_stats_global, on='hour_block', how='left')\n",
        "\n",
        "# Fill missing values\n",
        "for col in train.columns:\n",
        "    if train[col].dtype in ['float64', 'int64']:\n",
        "        train[col] = train[col].fillna(0)\n",
        "        test[col] = test[col].fillna(0)\n",
        "\n",
        "for col in ['comm_target_mode', 'dist_target_mode', 'hourblock_target_mode']:\n",
        "    if col in train.columns:\n",
        "        train[col] = train[col].fillna('NORAIN')\n",
        "        test[col] = test[col].fillna('NORAIN')\n",
        "\n",
        "print(f\"Final train shape: {train.shape}, test shape: {test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmwqaLjVaj_R"
      },
      "source": [
        "Encoding des variables catégorielles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tKSkrCpalmN"
      },
      "outputs": [],
      "source": [
        "print(\"Encoding categorical variables...\")\n",
        "\n",
        "for col in ['community', 'district', 'indicator']:\n",
        "    le = LabelEncoder()\n",
        "    train[col] = train[col].fillna('missing').astype(str)\n",
        "    test[col] = test[col].fillna('missing').astype(str)\n",
        "    all_vals = pd.concat([train[col], test[col]]).unique()\n",
        "    le.fit(all_vals)\n",
        "    train[col] = le.transform(train[col])\n",
        "    test[col] = le.transform(test[col])\n",
        "\n",
        "for col in ['comm_target_mode', 'dist_target_mode', 'hourblock_target_mode']:\n",
        "    if col in train.columns:\n",
        "        le = LabelEncoder()\n",
        "        all_vals = pd.concat([train[col], test[col]]).unique()\n",
        "        le.fit(all_vals)\n",
        "        train[col] = le.transform(train[col])\n",
        "        test[col] = le.transform(test[col])\n",
        "\n",
        "print(\"Encoding completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EMnBJbwan2R"
      },
      "source": [
        "Fonction d'agrégation leak-safe pour CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikrNVE9Yapk6"
      },
      "outputs": [],
      "source": [
        "def add_aggregations_cv(train_df, val_df, test_df):\n",
        "    \"\"\"Recalculate user statistics on training fold only (leak-safe)\"\"\"\n",
        "    user_stats = train_df.groupby('user_id').agg({\n",
        "        'confidence': ['mean', 'std', 'min', 'max', 'count'],\n",
        "        'predicted_intensity': ['mean', 'sum'],\n",
        "        'forecast_length': 'mean'\n",
        "    }).reset_index()\n",
        "    user_stats.columns = ['user_id'] + [f'user_{col[0]}_{col[1]}' for col in user_stats.columns[1:]]\n",
        "    user_stats['user_confidence_std'] = user_stats['user_confidence_std'].fillna(0)\n",
        "\n",
        "    for df in [val_df, test_df]:\n",
        "        user_cols_to_drop = [c for c in df.columns if c.startswith('user_') and c != 'user_id']\n",
        "        df.drop(columns=user_cols_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "    val_df = val_df.merge(user_stats, on='user_id', how='left')\n",
        "    test_df = test_df.merge(user_stats, on='user_id', how='left')\n",
        "\n",
        "    for col in user_stats.columns[1:]:\n",
        "        val_df[col] = val_df[col].fillna(0)\n",
        "        test_df[col] = test_df[col].fillna(0)\n",
        "\n",
        "    return val_df, test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJtpe7kEate5"
      },
      "source": [
        "Préparation du target et class weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VElSRTq2avEz"
      },
      "outputs": [],
      "source": [
        "target_encoder = LabelEncoder()\n",
        "y_train = target_encoder.fit_transform(train['Target'])\n",
        "n_classes = len(target_encoder.classes_)\n",
        "\n",
        "print(f\"Target classes: {target_encoder.classes_}\")\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "print(f\"Class weights: {class_weight_dict}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1Wzx9q9aw3o"
      },
      "source": [
        "Sélection des features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP6JO1xsayry"
      },
      "outputs": [],
      "source": [
        "drop_cols = ['ID', 'prediction_time', 'indicator_description', 'time_observed', 'Target']\n",
        "feature_cols = [col for col in train.columns if col not in drop_cols]\n",
        "\n",
        "print(f\"Number of features: {len(feature_cols)}\")\n",
        "print(f\"Sample features: {feature_cols[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJr3gCzTa0jf"
      },
      "source": [
        "Configuration du Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKekcflya3Ne"
      },
      "outputs": [],
      "source": [
        "n_splits = 10\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "\n",
        "oof_preds_lgb = np.zeros((len(train), n_classes))\n",
        "oof_preds_xgb = np.zeros((len(train), n_classes))\n",
        "oof_preds_cat = np.zeros((len(train), n_classes))\n",
        "\n",
        "test_preds_lgb = np.zeros((len(test), n_classes))\n",
        "test_preds_xgb = np.zeros((len(test), n_classes))\n",
        "test_preds_cat = np.zeros((len(test), n_classes))\n",
        "\n",
        "f1_scores = []\n",
        "\n",
        "print(f\"Stratified {n_splits}-Fold Cross-Validation configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0tWMooMa6oc"
      },
      "source": [
        " Entraînement des modèles (Boucle principale)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUutk0FAa8lK"
      },
      "outputs": [],
      "source": [
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train, y_train)):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Fold {fold + 1}/{n_splits}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    train_fold_raw = train_original.iloc[train_idx].copy()\n",
        "    val_fold = train.iloc[val_idx].copy()\n",
        "    test_fold = test.copy()\n",
        "\n",
        "    val_fold, test_fold = add_aggregations_cv(train_fold_raw, val_fold, test_fold)\n",
        "\n",
        "    X_tr = train.iloc[train_idx][feature_cols].fillna(-999)\n",
        "    X_val = val_fold[feature_cols].fillna(-999)\n",
        "    X_te = test_fold[feature_cols].fillna(-999)\n",
        "\n",
        "    y_tr = y_train[train_idx]\n",
        "    y_val = y_train[val_idx]\n",
        "\n",
        "    # LightGBM\n",
        "    print(\"\\nTraining LightGBM...\")\n",
        "    lgb_params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': n_classes,\n",
        "        'metric': 'multi_logloss',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': 0.01,\n",
        "        'num_leaves': 127,\n",
        "        'max_depth': 10,\n",
        "        'min_child_samples': 10,\n",
        "        'subsample': 0.7,\n",
        "        'colsample_bytree': 0.7,\n",
        "        'reg_alpha': 0.5,\n",
        "        'reg_lambda': 0.5,\n",
        "        'random_state': SEED,\n",
        "        'verbose': -1,\n",
        "        'is_unbalance': True\n",
        "    }\n",
        "\n",
        "    sample_weights_tr = np.array([class_weights[y] for y in y_tr])\n",
        "    sample_weights_val = np.array([class_weights[y] for y in y_val])\n",
        "\n",
        "    lgb_train = lgb.Dataset(X_tr, label=y_tr, weight=sample_weights_tr)\n",
        "    lgb_val = lgb.Dataset(X_val, label=y_val, weight=sample_weights_val, reference=lgb_train)\n",
        "\n",
        "    lgb_model = lgb.train(\n",
        "        lgb_params,\n",
        "        lgb_train,\n",
        "        num_boost_round=3000,\n",
        "        valid_sets=[lgb_train, lgb_val],\n",
        "        callbacks=[lgb.early_stopping(200), lgb.log_evaluation(500)]\n",
        "    )\n",
        "\n",
        "    oof_preds_lgb[val_idx] = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration)\n",
        "    test_preds_lgb += lgb_model.predict(X_te, num_iteration=lgb_model.best_iteration) / n_splits\n",
        "\n",
        "    # XGBoost\n",
        "    print(\"\\nTraining XGBoost...\")\n",
        "    xgb_params = {\n",
        "        'objective': 'multi:softprob',\n",
        "        'num_class': n_classes,\n",
        "        'eval_metric': 'mlogloss',\n",
        "        'learning_rate': 0.01,\n",
        "        'max_depth': 10,\n",
        "        'min_child_weight': 1,\n",
        "        'subsample': 0.7,\n",
        "        'colsample_bytree': 0.7,\n",
        "        'reg_alpha': 0.5,\n",
        "        'reg_lambda': 0.5,\n",
        "        'random_state': SEED,\n",
        "        'tree_method': 'hist'\n",
        "    }\n",
        "\n",
        "    sample_weights = np.array([class_weights[y] for y in y_tr])\n",
        "    dtrain = xgb.DMatrix(X_tr, label=y_tr, weight=sample_weights)\n",
        "    dval = xgb.DMatrix(X_val, label=y_val)\n",
        "    dtest = xgb.DMatrix(X_te)\n",
        "\n",
        "    xgb_model = xgb.train(\n",
        "        xgb_params,\n",
        "        dtrain,\n",
        "        num_boost_round=3000,\n",
        "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
        "        early_stopping_rounds=200,\n",
        "        verbose_eval=500\n",
        "    )\n",
        "\n",
        "    oof_preds_xgb[val_idx] = xgb_model.predict(dval)\n",
        "    test_preds_xgb += xgb_model.predict(dtest) / n_splits\n",
        "\n",
        "    # CatBoost\n",
        "    print(\"\\nTraining CatBoost...\")\n",
        "    cat_model = CatBoostClassifier(\n",
        "        iterations=3000,\n",
        "        learning_rate=0.01,\n",
        "        depth=10,\n",
        "        l2_leaf_reg=10,\n",
        "        auto_class_weights='Balanced',\n",
        "        random_state=SEED,\n",
        "        loss_function='MultiClass',\n",
        "        eval_metric='TotalF1:average=Macro',\n",
        "        early_stopping_rounds=200,\n",
        "        verbose=500,\n",
        "        task_type='CPU'\n",
        "    )\n",
        "\n",
        "    cat_model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=(X_val, y_val),\n",
        "        use_best_model=True\n",
        "    )\n",
        "\n",
        "    oof_preds_cat[val_idx] = cat_model.predict_proba(X_val)\n",
        "    test_preds_cat += cat_model.predict_proba(X_te) / n_splits\n",
        "\n",
        "    # Ensemble evaluation\n",
        "    oof_ensemble = (oof_preds_lgb[val_idx] * 0.35 +\n",
        "                    oof_preds_xgb[val_idx] * 0.35 +\n",
        "                    oof_preds_cat[val_idx] * 0.30)\n",
        "\n",
        "    fold_preds = np.argmax(oof_ensemble, axis=1)\n",
        "    fold_f1 = f1_score(y_val, fold_preds, average='macro')\n",
        "    f1_scores.append(fold_f1)\n",
        "\n",
        "    print(f\"\\nFold {fold + 1} Macro F1: {fold_f1:.6f}\")\n",
        "\n",
        "print(f\"\\nMean CV F1 Score: {np.mean(f1_scores):.6f} (+/- {np.std(f1_scores):.6f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6uX2PhCa_t-"
      },
      "source": [
        "Stacking Meta-Learner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ESbaqssbBcE"
      },
      "outputs": [],
      "source": [
        "print(\"Training stacking meta-learner...\")\n",
        "\n",
        "oof_stack = np.hstack([oof_preds_lgb, oof_preds_xgb, oof_preds_cat])\n",
        "meta_model = LogisticRegression(\n",
        "    multi_class='multinomial',\n",
        "    solver='lbfgs',\n",
        "    max_iter=2000,\n",
        "    C=0.1,\n",
        "    class_weight='balanced',\n",
        "    random_state=SEED\n",
        ")\n",
        "meta_model.fit(oof_stack, y_train)\n",
        "\n",
        "oof_meta_preds = meta_model.predict(oof_stack)\n",
        "oof_meta_f1 = f1_score(y_train, oof_meta_preds, average='macro')\n",
        "\n",
        "print(f\"Meta-learner F1 Score: {oof_meta_f1:.6f}\")\n",
        "print(f\"Base ensemble F1 Score: {np.mean(f1_scores):.6f}\")\n",
        "print(f\"Improvement: +{oof_meta_f1 - np.mean(f1_scores):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-VHXSQybEaZ"
      },
      "source": [
        "Prédictions finales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQnoQl9wbF4B"
      },
      "outputs": [],
      "source": [
        "test_stack = np.hstack([test_preds_lgb, test_preds_xgb, test_preds_cat])\n",
        "final_predictions = meta_model.predict(test_stack)\n",
        "final_predictions_labels = target_encoder.inverse_transform(final_predictions)\n",
        "\n",
        "print(\"Final predictions completed\")\n",
        "print(f\"Prediction distribution:\\n{pd.Series(final_predictions_labels).value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv_6xsA0bMLB"
      },
      "source": [
        "Résumé des résultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NZBeo5abOF3"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"MODEL TRAINING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Cross-validation folds: {n_splits}\")\n",
        "print(f\"Number of features: {len(feature_cols)}\")\n",
        "print(f\"Training samples: {len(train)}\")\n",
        "print(f\"Test samples: {len(test)}\")\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Mean CV F1: {np.mean(f1_scores):.6f}\")\n",
        "print(f\"  Std CV F1: {np.std(f1_scores):.6f}\")\n",
        "print(f\"  Meta F1: {oof_meta_f1:.6f}\")\n",
        "print(f\"\\nOptimizations applied:\")\n",
        "print(\"  - Cyclical time features\")\n",
        "print(\"  - TF-IDF text vectorization\")\n",
        "print(\"  - Leak-safe aggregations\")\n",
        "print(\"  - Stratified K-fold CV\")\n",
        "print(\"  - Ensemble of 3 models\")\n",
        "print(\"  - Stacking meta-learner\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
